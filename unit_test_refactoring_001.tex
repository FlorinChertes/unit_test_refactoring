\documentclass{article}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\usepackage{graphicx}

\title{unit test and refactoring}
\date{2022-06-18}
\author{Florin Ioan Chertes}

\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}


\begin{abstract}
Unit test and refactoring are processes that go hand in hand with the purpose fostering the creation of high source code quality. These both processes make the maintainability of the source code manageable and prepare it for reliable change.
\end{abstract}

\section{Introduction}

As Beck \cite{BecAnd04extreme, beck2001planning} explains, testing is one of the most important parts of the discipline Extreme Programming (\textit{XP}). In his view development is driven by tests, you test first, then you code. When all the tests run, and you can't think of any more tests that would break, you are done adding functionality. He mentions the importance of integration tests that follow immediately the development. Automated test give the possibility to defer the important design decisions as late as possible when the requirements are better understood. Feedback is one of the most important values of XP. Feedback works at different time scales. The programmers write unit tests for all the logic in program that could break. The customers write stories (description of features, simplified use-cases). The testers write functional testes, business functionality for all stories. So the unit tests, written by developers and the functional tests written by customers and testers are the hard of the \textit{XP}.   

\section{Unit Test}

\subsection{Unit Test in Detail}

Feather \cite{FeathersMichael} says that the concept of unit test is about the idea that they are tests in isolation of individual software components. But what are components? The definition could vary, but in unit tests we are usually concerned with the most atomic behavioral units of the system. In object oriented code the units are the classes.


\subsection{Modern \textit{unit test} concepts}

Khorikov \cite{Khoricov2020} introduced the \textit{unit test} based on the work of many practitioners and scholars for more than a quarter of century.

\begin{definition}
\begin{itemize}
Unit tests are automatic tests having the following properties:
	\item a unit test verifies a single unit of behavior,
	\item does it quickly and
	\item does it in isolation from other tests.
\end{itemize}
\end{definition}


The \textit{isolation issue} is the most disputed. The dispute led to the formation of two schools of unit testing: the classical (Detroit) school, and the London (\textit{mockist}) school. This difference of opinion affects the view of what constitutes a unit and the treatment of the system under test’s (SUT’s) dependencies.

\begin{itemize}
	\item The London school states that \textit{the units under test} should be isolated from each other. A unit under test is a unit of code, usually a class. All of its dependencies, except immutable dependencies, should be replaced with test doubles in tests.
	\item The classical school states that \textit{the unit tests} need to be isolated from each other, not \textit{the units under test}. Also, \textit{a unit under test} is a unit of behavior, not a unit of code. Thus, only shared dependencies should be replaced with test doubles. Shared dependencies are dependencies that provide means for tests to affect each others execution flow.
\end{itemize}

The London school provides the benefits of better granularity, the ease of testing large graphs of interconnected classes, and the ease of finding which functionality contains a bug after a test failure.

The benefits of the London school look appealing at first. However, they introduce several issues. \textit{First}, the focus on classes under test is misplaced: tests should verify units of behavior, not units of code. \textit{Furthermore}, the inability to unit test a piece of code is a strong sign of a problem with the code design. The use of test doubles doesn’t fix this problem, but rather only hides it. And \textit{finally}, while the ease of determining which functionality contains a bug after a test failure is helpful, it’s not that big a deal because you often know what caused the bug anyway—it’s what you edited last.

The biggest issue with the London school of unit testing is the problem of over specification—coupling tests to the SUT’s implementation details.

\begin{definition}
An integration test is a test that doesn’t meet at least one of the criteria for a unit test. 
\end{definition}

\begin{definition}
End-to-end tests are a subset of integration tests; they verify the system from the end user’s point of view. End-to-end tests reach out directly to all or almost all out-of-process dependencies your application works with.
\end{definition}


\subsection{Are the classes or the modules the support of the behavior?}

The Test-driven design \cite{beck2002driven} was wrong understood and used, says Cooper \cite{WEBSITE:WhereDidItAllGoWrong}. Heinemeier Hansson \cite{WEBSITE:TDDisdead} even says that \textit{TDD} is dead. They both go back to the roots of \textit{TDD} and agree that: 

\begin{itemize}
	\item test behavior, this is the stable contract of the \textit{API}, 
	\item do not test implementation details, these details can change very rapidly
	\item test the implementation details only when you need to understand the refactoring of the implementation, afterwords delete these tests.
\end{itemize}

The common \textit{TDD} practice by using: 'adding a new method to a class' as the trigger to write a test, is simply wrong. Adding a new class is not the trigger for writing tests. The trigger is implementing a requirement.
The stable part of the implementation is its public exposed API of the module, this must be tested. Write test to cover the use cases or stories, use the model: \textit{Given}, \textit{When}, \textit{Then}. The subject under test \textit{SUT} is not a class, \textit{SUT} is the export from a module, its facade.
Unit means a module not a class, modules consist of many classes that constitute the implementation detail.

In Beck \cite{beck2002driven}, is explained, that the object of \textit{TDD} is to test behavior in the system. When it runs in isolation, is the test not a class using mocks to be isolated. The test is isolated not the \textit{SUT}.
The test are:
\begin{itemize}
	\item unit test of the module,
	\item integration test and
	\item system test.
\end{itemize}

If is it difficult to use file system, database, etc., in the unit tests, do not mock them, but fixtures to put your unit tests to use the file system, database, etc. Writing tests that target a method on a class, is not a \textit{TDD} developer test. \textit{TDD} unit tests focus on a story, use-case, scenario, etc. Unit test focusing on method are hard to maintain, these tests do not capture the behavior they want to preserve, detail implementation exposed to unit tests make refactoring difficult. In the \textit{TDD} cycle when you are ready with the implementation, you do not write new unit tests, when refactoring to clean code.

Eliminate the dependency between tests an production code. Tests must not depend on implementation details, because changing the implementation, breaks the tests. Tests must depend on contracts, or public interfaces. This allows for refactor implementation without changing tests. Do not test implementation details, they change quickly. Do not use Mocks to confirm implementation details.

Coplien \cite{WEBSITE:UnitTestIsWaste} considers unit tests a waste. He consider to automate:
\begin{itemize}
	\item integration tests, 
	\item bug regression tests and 
	\item system tests 
\end{itemize}
rather than automating unit tests. 

A smarter approach would reduce the test code mass through formal test design: that is, to do formal boundary-condition checking, more white-box testing, and so forth. That requires that the unit under test be designed for test-ability. This is how hardware engineers do it: designers provide "test points" that can read out values on a J-Tag pin of a chip, to access internal signal values of the chip — tantamount to accessing values between intermediate computations in a computational unit. I advocate doing this at the system level where the testing focus should lie; I have never seen anyone achieve this at the unit level. Without such hooks you are limited to black-box unit testing. 

Tests should be designed with great care. Business people, rather than programmers, should design most functional tests. Unit tests should be limited to those that can be held up against some “third-party” success criteria.

In Coplien \cite{WEBSITE:UnitTestIsWaste} we find the description of a project in which the developers had written their tests in such a way, that they didn't have to change the tests when the functionality changed. That of course means that the tests  weren't testing the functionality, so whatever they were testing was of little value. Those have no provable value. There were methodologies in the 1970s and 1980s based on trace-ability that tried to reduce system requirements all the way down to the unit level. In general, that's an NP-hard problem (unless you are doing pure procedural decomposition) so I'm very skeptical of anyone who says they can do that. So one question to ask about every test is: If this test fails, what business requirement is compromised? Most of the time, the answer is, "I don't know." If you don't know the value of the test, then the test theoretically could have zero business value. The test does have a cost: maintenance, computing time, administration, and so forth. That means the test could have net negative value. That is the fourth category of tests to remove. These are tests which, though they may even do some amount of verification, do no validation.

In summary: 
\begin{itemize}
	\item Keep regression tests around for up to a year — but most of those will be system-level tests rather than unit tests. 
	\item Keep unit tests that test key algorithms for which there is a broad, formal, independent oracle of correctness, and for which there is ascribable business value.
	\item  Except for the preceding case, if X has business value and you can test X with either a system test or a unit test, use a system test — context is everything.
	\item Design a test with more care than you design the code.
	\item Turn most unit tests into assertions.
	\item Throw away tests that haven’t failed in a year.
	\item Testing can’t replace good development: a high test failure 
rate suggests you should shorten development intervals, 
perhaps radically, and make sure your architecture and design 
regimens have teeth
	\item If you find that individual functions being tested are trivial, 
double-check the way you incentivize developers’ 
performance. Rewarding coverage or other meaningless 
metrics can lead to rapid architecture decay.
	\item Be humble about what tests can achieve. Tests don’t improve 
quality: developers do
\end{itemize}

\subsection{\textit{TDD} and Software Design Quality}

Test-driven development creates software \cite{TestDrivenDevelopmentConceptsTaxonomy} in very short iterations with minimal upfront design. Poised for widespread adoption, TDD has become the focus of an increasing number of researchers and developers.

Support for test-driven development (TDD) is growing in many development contexts beyond its common association with extreme programming.  We \cite{TestDrivenDevelopmentReallyImproveSoftwareDesign} weren't able to confirm claims that TDD improves cohesion while lowering coupling, but we anticipate ways to clarify the questions these design characteristics raised. In particular, we're working to eliminate the confounding factor of accessor usage in the cohesion metrics.

It is suggested \cite{DoesTestDrivenDevelopmentImprovetheProgramCode} that test-driven development (TDD) is one of the most fundamental practices in agile software development, which produces loosely coupled and highly cohesive code. However, how the TDD impacts on the structure of the program code have not been widely studied. This paper presents the results from a comparative case study of five small scale software development projects where the effect of TDD on program design was studied using both traditional and package level metrics. The empirical results reveal that an unwanted side effect can be that some parts of the code may deteriorate. In addition, the differences in the program code, between TDD and the iterative test-last development, were not as clear as expected. This raises the question as to whether the possible benefits of TDD are greater than the possible downsides. Moreover, it additionally questions whether the same benefits could be achieved just by emphasizing unit-level testing activities.

\section{next Section }

Next section

\subsection{Next subsection}

Next subsection

\section{Conclusion}

Conclusion




\newpage

\bibliography{unit_test_refactoring_001} 
\bibliographystyle{ieeetr}


\end{document}




